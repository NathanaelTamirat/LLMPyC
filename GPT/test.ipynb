{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0.],\n",
       "        [1., 1., 0., 0.],\n",
       "        [1., 1., 1., 0.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 0., 0., 0.],\n",
       "          [1., 1., 0., 0.],\n",
       "          [1., 1., 1., 0.],\n",
       "          [1., 1., 1., 1.]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(4,4)).view(1,1,4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "FLASH=0\n",
    "\n",
    "class CasualSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.c_proj.LLMC_RESIDUAL_SCALE_FLAG = 1\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                             .view(1, 1, config.block_size, config.block_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        print(f\"Input size: B={B}, T={T}, C={C}\")\n",
    "\n",
    "        qkv = self.c_attn(x)\n",
    "        print(f\"QKV size: {qkv.size()}\")\n",
    "\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        print(f\"Q size: {q.size()}, K size: {k.size()}, V size: {v.size()}\")\n",
    "\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        print(f\"Reshaped Q size: {q.size()}, K size: {k.size()}, V size: {v.size()}\")\n",
    "\n",
    "        if FLASH:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            print(f\"Attention scores size: {att.size()}\")\n",
    "\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            print(f\"Softmaxed attention scores size: {att.size()}\")\n",
    "\n",
    "            y = att @ v\n",
    "            print(f\"Attention output size: {y.size()}\")\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        print(f\"Reshaped output size: {y.size()}\")\n",
    "\n",
    "        y = self.c_proj(y)\n",
    "        print(f\"Final output size: {y.size()}\")\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: B=2, T=128, C=64\n",
      "QKV size: torch.Size([2, 128, 192])\n",
      "Q size: torch.Size([2, 128, 64]), K size: torch.Size([2, 128, 64]), V size: torch.Size([2, 128, 64])\n",
      "Reshaped Q size: torch.Size([2, 8, 128, 8]), K size: torch.Size([2, 8, 128, 8]), V size: torch.Size([2, 8, 128, 8])\n",
      "Attention scores size: torch.Size([2, 8, 128, 128])\n",
      "Softmaxed attention scores size: torch.Size([2, 8, 128, 128])\n",
      "Attention output size: torch.Size([2, 8, 128, 8])\n",
      "Reshaped output size: torch.Size([2, 128, 64])\n",
      "Final output size: torch.Size([2, 128, 64])\n",
      "Output: tensor([[[ 0.2121,  0.0830, -0.3663,  ..., -0.4172,  0.0820,  0.0012],\n",
      "         [ 0.3427,  0.2058, -0.4499,  ..., -0.2930, -0.0223,  0.0244],\n",
      "         [ 0.3764,  0.2091, -0.5816,  ..., -0.3658,  0.0075,  0.0493],\n",
      "         ...,\n",
      "         [ 0.0141, -0.1585, -0.1081,  ...,  0.0640, -0.1367,  0.2067],\n",
      "         [-0.0184, -0.1475, -0.1249,  ...,  0.0404, -0.1119,  0.1863],\n",
      "         [-0.0016, -0.1797, -0.0994,  ...,  0.0016, -0.0404,  0.1661]],\n",
      "\n",
      "        [[-0.9531, -0.4266,  0.5857,  ..., -0.0971, -0.4274,  0.4366],\n",
      "         [-0.5407, -0.2379,  0.1678,  ..., -0.0643, -0.3364,  0.0757],\n",
      "         [-0.2621, -0.0367,  0.0398,  ...,  0.0736, -0.2680, -0.0505],\n",
      "         ...,\n",
      "         [ 0.0239, -0.1756, -0.0961,  ..., -0.0457, -0.0675,  0.1331],\n",
      "         [ 0.0132, -0.1406, -0.0956,  ..., -0.0495, -0.0707,  0.1130],\n",
      "         [ 0.0444, -0.1759, -0.0912,  ..., -0.0357, -0.0923,  0.1938]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define a simple configuration class\n",
    "class Config:\n",
    "    def __init__(self, n_embd, n_head, block_size, attn_pdrop, resid_pdrop):\n",
    "        self.n_embd = n_embd\n",
    "        self.n_head = n_head\n",
    "        self.block_size = block_size\n",
    "        self.attn_pdrop = attn_pdrop\n",
    "        self.resid_pdrop = resid_pdrop\n",
    "\n",
    "# Create a configuration object\n",
    "config = Config(\n",
    "    n_embd=64,      # Embedding dimension\n",
    "    n_head=8,       # Number of attention heads\n",
    "    block_size=128, # Maximum sequence length\n",
    "    attn_pdrop=0.1, # Dropout probability for attention\n",
    "    resid_pdrop=0.1 # Dropout probability for residuals\n",
    ")\n",
    "\n",
    "# Instantiate the CasualSelfAttention model\n",
    "model = CasualSelfAttention(config)\n",
    "\n",
    "# Create a sample input tensor (batch size, sequence length, embedding dimension)\n",
    "x = torch.randn(2, 128, 64)  # Example with batch size 2, sequence length 128, embedding dimension 64\n",
    "\n",
    "# Run the model\n",
    "output = model(x)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
